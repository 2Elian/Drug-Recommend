import json
import torch

def load_drug_vocab(drug_file_path: str):
    with open(drug_file_path, 'r', encoding='utf-8') as f:
        drugs = json.load(f)
    drug_to_idx = {drug: idx for idx, drug in enumerate(drugs)}
    """
    example
    # {
    #     "左甲状腺素钠片": 0,
    #     "氨氯地平片": 1, 
    #     "阿卡波糖": 2,
    #     "瑞格列奈": 3,
    #     ...
    # }
    """
    idx_to_drug = {idx: drug for drug, idx in drug_to_idx.items()}
    """
    example
    # {
    #     0: "左甲状腺素钠片",
    #     1: "氨氯地平片",
    #     2: "阿卡波糖", 
    #     3: "瑞格列奈",
    #     ...
    # }
    """
    return drug_to_idx, idx_to_drug, drugs

def create_multihot_labels(drug_list: list, drug_to_idx: dict, num_drugs: int):
    labels = torch.zeros(num_drugs, dtype=torch.float) # [0, 0, 0, 0, ...]
    for drug in drug_list:
        if drug in drug_to_idx:
            labels[drug_to_idx[drug]] = 1.0
    return labels # example [1, 1, 0, 0, 1, 0, ...]

def build_medical_lm_prompt(data: dict) -> str:
    sections = []
    basic_info = []
    gender = data.get("性别")
    if gender is not None:
        basic_info.append(f"[性别]: {gender},")
    birth_date = data.get("出生日期")
    if birth_date is not None:
        basic_info.append(f"[出生日期]: {birth_date},")
    bmi = data.get("BMI")
    if bmi is not None:
        bmi_status = "肥胖" if bmi >= 28 else "超重" if bmi >= 24 else "正常"
        basic_info.append(f"[BMI]: {bmi}（{bmi_status}）,")
    ethnicity = data.get("民族")
    if ethnicity is not None:
        basic_info.append(f"[民族]: {ethnicity}。")
    if basic_info:
        sections.append("【患者基本信息】\n" + "".join(basic_info) + "\n 【患者临床信息】")
    past_history = data.get("既往史")
    if past_history is not None and past_history.strip():
        sections.append("[既往史]\n" + past_history.strip())
    present_illness = data.get("现病史")
    if present_illness is not None and present_illness.strip():
        sections.append("[现病史]\n" + present_illness.strip())
    chief_complaint = data.get("主诉")
    if chief_complaint is not None and chief_complaint.strip():
        sections.append("[患者主诉]\n" + chief_complaint.strip())
    admission_status = data.get("入院情况")
    if admission_status is not None and admission_status.strip():
        sections.append("[入院情况]\n" + admission_status.strip())
    process_desc = data.get("诊疗过程描述")
    if process_desc is not None and process_desc.strip():
        clean_process = process_desc.strip()
        sections.append("[诊疗过程]\n" + clean_process)
    
    diagnoses = data.get("出院诊断")
    if diagnoses is not None and isinstance(diagnoses, list):
        valid_diagnoses = []
        for diagnosis in diagnoses:
            if diagnosis is not None:
                if isinstance(diagnosis, str) and diagnosis.strip():
                    valid_diagnoses.append(diagnosis.strip())
                elif isinstance(diagnosis, (int, float)):
                    valid_diagnoses.append(str(diagnosis))
        
        if valid_diagnoses:
            sections.append("【出院诊断】\n" + ":".join(valid_diagnoses))
    if sections:
        prompt = "\n\n".join(sections)
    else:
        raise
    
    instruction = """# 你是一名专业的临床医生，你的任务是针对患者的【基础信息】、【临床信息】和【出院诊断】给出患者出院时需要携带的药物列表。"""
    final_text =  instruction + "\n" + prompt + "\n" + "请推荐药物："
    
    return final_text

def drug_classification_map_fn_optimized(data: dict, tokenizer, max_seq_length: int, special_drug_tokens: list, is_train: bool = True):
    """
    Optimized mapping function for training and inference (evaluation/prediction).
    
    Args:
        data (dict): The input sample dictionary (e.g., medical record).
        tokenizer: The pre-trained tokenizer.
        max_seq_length (int): Maximum sequence length for truncation.
        special_drug_tokens (list): List of special drug token strings (e.g., ['<DRUG_X>']).
        is_train (bool): If True, prepares 'labels'; otherwise, omits 'labels'.
    """
    tokenizer.truncation_side = 'left'
    # 1. Build the base prompt text
    input_text = build_medical_lm_prompt(data)
    
    # 2. Append target drug tokens only during training
    if is_train:
        # Include the ground truth drugs for the model to learn the sequence
        drug_list = data.get("出院带药列表", [])
        drug_tokens = [f"<DRUG_{d}>" for d in drug_list]
        drug_text = "".join(drug_tokens)
        input_text += f"\n{drug_text}"
        
    # 3. Encode the text
    # Note: Using tokenizer() is generally preferred over tokenizer.encode()
    # as it allows for cleaner handling of truncation/padding/masks later.
    encoded_input = tokenizer(
        input_text, 
        max_length=max_seq_length, 
        truncation=True, 
        padding=False
    )
    
    input_ids = encoded_input["input_ids"]
    attention_mask = encoded_input["attention_mask"] # Use the mask generated by the tokenizer

    result = {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
    }

    # 4. Label Generation (Only during training)
    if is_train:
        # Initialize labels: -100 means ignore in loss calculation
        lm_labels = [-100] * len(input_ids)
        
        # Get the IDs of the special drug tokens
        special_token_ids_set = set(tokenizer.convert_tokens_to_ids(special_drug_tokens))
        
        # Find the index of the first special drug token
        start_idx = None
        for i, token_id in enumerate(input_ids):
            # We check if the token ID is one of the special drug IDs
            if token_id in special_token_ids_set:
                start_idx = i
                break
        
        # Set labels for the output sequence (the drug tokens)
        if start_idx is not None:
            # Shift labels to the actual input IDs for loss calculation
            lm_labels[start_idx:] = input_ids[start_idx:]
        else:
            # If no drug token is found, this sample is invalid for training
            # A common practice is to let the filter function handle this before mapping.
            # If used within map, it should rely on filtering the output.
            raise KeyError("Drug special token not found in the input sequence.")
        
        result["labels"] = lm_labels
    return result

if __name__ == '__main__':
    from transformers import AutoTokenizer
    
    tokenizer = AutoTokenizer.from_pretrained(
        '/data1/nuist_llm/TrainLLM/ModelCkpt/glm/glm4-8b-chat', 
        trust_remote_code=True
    )
    drug_to_idx , _ , drug_list= load_drug_vocab('/data/lzm/DrugRecommend/src/data/pre_drug.json')
    special_drug_tokens = [f"<DRUG_{d}>" for d in drug_list]
    tokenizer.add_special_tokens({"additional_special_tokens": special_drug_tokens})
    jsonl_file = '/data/lzm/DrugRecommend/src/data/CDrugRed-A-v1/CDrugRed_train.jsonl'
    with open(jsonl_file, 'r', encoding='utf-8') as f:
        # 使用 enumerate(f, 1) 同时获取行内容和从 1 开始的行号
        for line_number, line in enumerate(f, 1):
            
            # 1. 跳过空行和纯空白行
            stripped_line = line.strip()
            if not stripped_line:
                continue
            data = json.loads(stripped_line)
            result = drug_classification_map_fn_optimized(data, tokenizer, 2500, special_drug_tokens, False)
            print("-"*100)
            print(result["input_ids"])
            print(result["attention_mask"])
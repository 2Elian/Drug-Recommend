train_file: /data/lzm/DrugRecommend/src/worker/dataset/train.jsonl
drug_file: /data/lzm/DrugRecommend/src/worker/dataset/pre_drug.json
output_dir: /data/lzm/DrugRecommend/resource/output/checkpoint_save/baseline_1106_xtuner
evaluation_strategy: steps
model_name_or_path: /data1/nuist_llm/TrainLLM/ModelCkpt/glm/glm4-8b-chat
max_seq_length: 4000
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
learning_rate: 2e-6
num_train_epochs: 2
use_focal_loss: true
use_lora: true
fp16: true
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.1
logging_steps: 50
save_steps: 200
save_total_limit: 3
use_deepspeed: true
is_train: true
distributed: true
